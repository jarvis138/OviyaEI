#!/usr/bin/env python3
"""
Test Maya-Level Enhancements
Tests the new prosodic markup, emotional memory, and audio post-processing systems
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))

from brain.llm_brain import OviyaBrain, ProsodyMarkup, EmotionalMemory
from voice.audio_postprocessor import AudioPostProcessor
from voice.emotion_library import get_emotion_library
import torch
import torchaudio
import numpy as np
import json


def test_prosodic_markup():
    """Test prosodic markup generation"""
    print("üé≠ Testing Prosodic Markup System")
    print("=" * 60)
    
    # Test different emotions with markup
    test_cases = [
        ("I'm so happy for you!", "joyful_excited", 0.8),
        ("I understand how you feel.", "empathetic_sad", 0.7),
        ("Take a deep breath... everything will be okay.", "calm_supportive", 0.9),
        ("You've got this! I believe in you completely.", "encouraging", 0.8),
        ("Oh yeah, that's exactly what I meant.", "sarcastic", 0.6),
    ]
    
    for text, emotion, intensity in test_cases:
        prosodic_text = ProsodyMarkup.add_prosodic_markup(text, emotion, intensity)
        print(f"\n{emotion} (intensity: {intensity}):")
        print(f"  Original: {text}")
        print(f"  Prosodic: {prosodic_text}")
    
    print("\n‚úÖ Prosodic markup test complete\n")


def test_emotional_memory():
    """Test emotional memory system"""
    print("üß† Testing Emotional Memory System")
    print("=" * 60)
    
    memory = EmotionalMemory()
    
    # Simulate conversation with changing emotions
    conversation = [
        ("joyful_excited", 0.8, "User shares good news"),
        ("empathetic_sad", 0.7, "User mentions problem"),
        ("comforting", 0.9, "Oviya responds supportively"),
        ("encouraging", 0.8, "Oviya motivates user"),
        ("calm_supportive", 0.6, "Conversation winds down")
    ]
    
    print("Simulating conversation emotional flow:\n")
    
    for i, (emotion, intensity, context) in enumerate(conversation, 1):
        state = memory.update(emotion, intensity)
        modifiers = memory.get_contextual_modifiers()
        
        print(f"Turn {i}: {context}")
        print(f"  Emotion: {emotion} (intensity: {intensity})")
        print(f"  Energy: {state['energy_level']:.2f}")
        print(f"  Pace: {state['pace']:.2f}")
        print(f"  Warmth: {state['warmth']:.2f}")
        print(f"  Mood: {state['conversation_mood']}")
        print()
    
    print("‚úÖ Emotional memory test complete\n")


def test_brain_integration():
    """Test brain with new prosodic and memory systems"""
    print("üß† Testing Enhanced Brain System")
    print("=" * 60)
    
    # Initialize brain (will use mock responses since Ollama might not be available)
    brain = OviyaBrain()
    
    # Test conversation flow
    test_messages = [
        ("I just got promoted at work!", "joyful_excited"),
        ("But I'm worried about the new responsibilities.", "concerned_anxious"),
        ("Do you think I can handle it?", "neutral")
    ]
    
    print("Testing conversation with emotional memory:\n")
    
    for i, (message, user_emotion) in enumerate(test_messages, 1):
        print(f"Turn {i}:")
        print(f"  User: {message} (emotion: {user_emotion})")
        
        # Get brain response
        response = brain.think(message, user_emotion=user_emotion)
        
        print(f"  Oviya: {response['text']}")
        print(f"  Emotion: {response['emotion']} (intensity: {response['intensity']})")
        
        if 'prosodic_text' in response:
            print(f"  üé≠ Prosodic: {response['prosodic_text']}")
        
        if 'emotional_state' in response:
            state = response['emotional_state']
            print(f"  üß† Memory: energy={state['energy_level']:.2f}, pace={state['pace']:.2f}")
        
        print()
    
    print("‚úÖ Enhanced brain test complete\n")


def test_audio_postprocessor():
    """Test audio post-processing system"""
    print("üéöÔ∏è Testing Audio Post-Processor")
    print("=" * 60)
    
    # Create test audio (sine wave)
    sample_rate = 24000
    duration = 2.0
    frequency = 440
    
    t = torch.linspace(0, duration, int(sample_rate * duration))
    test_audio = 0.3 * torch.sin(2 * np.pi * frequency * t)
    
    # Initialize processor
    processor = AudioPostProcessor(sample_rate)
    
    # Test different prosodic texts
    test_cases = [
        {
            "name": "Basic speech",
            "prosodic_text": "Hello there!",
            "emotional_state": {"energy_level": 0.5, "pace": 1.0, "warmth": 0.5}
        },
        {
            "name": "Excited with breath",
            "prosodic_text": "<breath> That's amazing! <smile> I'm so happy for you! <breath>",
            "emotional_state": {"energy_level": 0.9, "pace": 1.2, "warmth": 0.8}
        },
        {
            "name": "Calm with pauses",
            "prosodic_text": "<breath> Take a deep breath... <pause> Everything will be okay. <breath>",
            "emotional_state": {"energy_level": 0.3, "pace": 0.8, "warmth": 0.9}
        }
    ]
    
    output_dir = Path("output")
    output_dir.mkdir(exist_ok=True)
    
    # Save original
    torchaudio.save(str(output_dir / "maya_test_original.wav"), test_audio.unsqueeze(0), sample_rate)
    print(f"üíæ Saved original: {len(test_audio)} samples ({len(test_audio)/sample_rate:.2f}s)")
    
    for i, test_case in enumerate(test_cases):
        print(f"\nProcessing: {test_case['name']}")
        print(f"  Prosodic: {test_case['prosodic_text']}")
        
        # Process audio
        processed = processor.process(
            test_audio.clone(),
            prosodic_text=test_case['prosodic_text'],
            emotional_state=test_case['emotional_state']
        )
        
        # Save processed audio
        filename = f"maya_test_{i+1}_{test_case['name'].lower().replace(' ', '_')}.wav"
        torchaudio.save(str(output_dir / filename), processed.unsqueeze(0), sample_rate)
        
        print(f"  ‚úÖ Processed: {len(processed)} samples ({len(processed)/sample_rate:.2f}s)")
        print(f"  üíæ Saved: {filename}")
    
    print(f"\n‚úÖ Audio post-processor test complete")
    print(f"üìÅ All files saved to: {output_dir}/\n")


def test_emotion_library_integration():
    """Test emotion library with new system"""
    print("üìö Testing Emotion Library Integration")
    print("=" * 60)
    
    library = get_emotion_library()
    
    # Test emotion resolution with prosodic patterns
    test_emotions = ["happy", "sad", "worried", "excited", "calm"]
    
    for emotion_input in test_emotions:
        resolved = library.get_emotion(emotion_input)
        tier = library.get_tier(resolved)
        
        # Test prosodic markup for this emotion
        test_text = "I understand how you're feeling right now."
        prosodic_text = ProsodyMarkup.add_prosodic_markup(test_text, resolved, 0.7)
        
        print(f"\n{emotion_input} ‚Üí {resolved} ({tier}):")
        print(f"  Prosodic: {prosodic_text}")
    
    print("\n‚úÖ Emotion library integration test complete\n")


def generate_comparison_samples():
    """Generate before/after comparison samples"""
    print("üéß Generating Before/After Comparison Samples")
    print("=" * 60)
    
    # This would require the full pipeline to be running
    # For now, just show what would be generated
    
    test_scenarios = [
        {
            "user_input": "I'm feeling really stressed about work",
            "expected_emotion": "concerned_anxious",
            "expected_prosodic": "<breath> I understand... <pause> Work stress can be really overwhelming. <gentle>You</gentle> don't have to handle this alone. <breath>"
        },
        {
            "user_input": "I just achieved my biggest goal!",
            "expected_emotion": "joyful_excited", 
            "expected_prosodic": "That's incredible! <smile> I'm so proud <smile> of you! <breath> You worked so hard for this!"
        },
        {
            "user_input": "I don't know what to do anymore",
            "expected_emotion": "empathetic_sad",
            "expected_prosodic": "<breath> I hear you... <long_pause> Sometimes life feels overwhelming. <gentle>You're</gentle> not alone in this. <breath>"
        }
    ]
    
    print("Expected Maya-level responses:\n")
    
    for i, scenario in enumerate(test_scenarios, 1):
        print(f"Scenario {i}:")
        print(f"  User: {scenario['user_input']}")
        print(f"  Emotion: {scenario['expected_emotion']}")
        print(f"  Prosodic: {scenario['expected_prosodic']}")
        print()
    
    print("‚úÖ Comparison scenarios defined\n")


def main():
    """Run all Maya enhancement tests"""
    print("\n" + "=" * 80)
    print("üé≠ MAYA-LEVEL ENHANCEMENT TEST SUITE")
    print("=" * 80 + "\n")
    
    try:
        # Test individual components
        test_prosodic_markup()
        test_emotional_memory()
        test_brain_integration()
        test_audio_postprocessor()
        test_emotion_library_integration()
        generate_comparison_samples()
        
        # Summary
        print("=" * 80)
        print("üìä MAYA ENHANCEMENT TEST RESULTS")
        print("=" * 80)
        print("‚úÖ Prosodic markup system: WORKING")
        print("‚úÖ Emotional memory system: WORKING") 
        print("‚úÖ Enhanced brain integration: WORKING")
        print("‚úÖ Audio post-processor: WORKING")
        print("‚úÖ Emotion library integration: WORKING")
        print("\nüéâ All Maya-level enhancements are ready!")
        print("\nüìã Next Steps:")
        print("   1. Deploy to Vast.ai with expanded emotions")
        print("   2. Test full pipeline with real CSM")
        print("   3. Compare audio quality before/after")
        print("   4. Optional: Integrate BigVGAN vocoder for even better quality")
        print("=" * 80)
        
    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
