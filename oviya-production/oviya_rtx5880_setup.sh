#!/bin/bash
# Oviya EI + WhisperX Production Setup for RTX 5880 Ada
# Run this script on your VastAI instance

echo "ğŸš€ Starting Oviya EI + WhisperX Production Setup on RTX 5880 Ada"
echo "=================================================================="

# Check GPU
echo "ğŸ” Checking GPU..."
nvidia-smi
echo ""

# Update system
echo "ğŸ“¦ Updating system packages..."
apt-get update -y
apt-get install -y curl wget git build-essential ffmpeg

# Install Python dependencies
echo "ğŸ Installing Python dependencies..."
pip install --upgrade pip
pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install whisperx
pip install git+https://github.com/snakers4/silero-vad.git
pip install sounddevice
pip install requests numpy soundfile huggingface-hub transformers librosa scipy flask

# Install Ollama
echo "ğŸ§  Installing Ollama..."
if ! command -v ollama &> /dev/null; then
    curl -fsSL https://ollama.ai/install.sh | sh
    echo "âœ… Ollama installed"
else
    echo "âœ… Ollama already installed"
fi

# Start Ollama service
echo "ğŸ”„ Starting Ollama service..."
ollama serve &
OLLAMA_PID=$!
sleep 15

# Pull Qwen2.5:7B model
echo "ğŸ“¥ Pulling Qwen2.5:7B model..."
ollama pull qwen2.5:7b
echo "âœ… Qwen2.5:7B model ready"

# Setup workspace
echo "ğŸ“ Setting up workspace..."
cd /workspace
mkdir -p oviya-production
cd oviya-production

# Create necessary directories
mkdir -p data/voice_samples
mkdir -p output
mkdir -p logs
mkdir -p emotion_references
mkdir -p external

# Download WhisperX models (cache them)
echo "ğŸ“¥ Caching WhisperX models..."
python3 -c "
import whisperx
import torch
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f'Caching WhisperX models on {device}')
print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB')
model = whisperx.load_model('large-v2', device, compute_type='float16')
align_model, metadata = whisperx.load_align_model('en', device)
print('âœ… WhisperX models cached successfully')
"

# Generate emotion references
echo "ğŸ­ Generating emotion references..."
python3 -c "
import torch
import torchaudio
from pathlib import Path

emotions = {
    'calm_supportive': 'Take a deep breath. Everything will be okay.',
    'empathetic_sad': 'I am so sorry you are going through this.',
    'joyful_excited': 'That is amazing! I am so happy for you!',
    'playful': 'Hey there! This is going to be fun!',
    'confident': 'You have got this. I believe in you.',
    'concerned_anxious': 'Are you okay? I am here if you need me.',
    'angry_firm': 'That is not acceptable. This needs to stop.',
    'neutral': 'Hello. How can I help you today?'
}

output_dir = Path('/workspace/oviya-production/emotion_references')
output_dir.mkdir(exist_ok=True)

for emotion, text in emotions.items():
    duration = 2.0
    sample_rate = 24000
    num_samples = int(duration * sample_rate)
    t = torch.linspace(0, duration, num_samples)
    
    # Generate synthetic reference with emotion-specific characteristics
    base_freq = 220
    audio = 0.3 * torch.sin(2 * torch.pi * base_freq * t)
    
    # Add emotion-specific modulation
    if emotion == 'joyful_excited':
        vibrato = 0.1 * torch.sin(2 * torch.pi * 5 * t)
        audio = audio * (1 + vibrato)
    elif emotion == 'empathetic_sad':
        decay = torch.exp(-t * 0.5)
        audio = audio * decay
    elif emotion == 'confident':
        audio = audio * 1.2  # Slightly louder
    
    output_path = output_dir / f'{emotion}.wav'
    torchaudio.save(str(output_path), audio.unsqueeze(0), sample_rate)
    print(f'Generated {emotion}.wav')

print('âœ… Emotion references generated successfully')
"

# Create CSM server script
echo "ğŸ¤ Creating CSM server script..."
cat > /workspace/oviya-production/csm_server_rtx5880.py << 'EOF'
#!/usr/bin/env python3
"""
CSM Server optimized for RTX 5880 Ada (48GB VRAM)
"""

import torch
import torchaudio
from flask import Flask, request, jsonify
import io
import base64
import os
from pathlib import Path
import json

# Set environment variables
os.environ["NO_TORCH_COMPILE"] = "1"

app = Flask(__name__)

# Check GPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸš€ Starting CSM Server on {device}")
print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")

# Load CSM model (you'll need to adapt this to your CSM implementation)
try:
    # This is a placeholder - replace with your actual CSM loading code
    print("ğŸ“¥ Loading CSM model...")
    # generator = load_csm_1b(device=device)  # Replace with your CSM loading
    print("âœ… CSM model loaded successfully!")
except Exception as e:
    print(f"âŒ CSM model loading failed: {e}")
    print("Please ensure CSM model files are available")

EMOTION_REF_DIR = Path("/workspace/oviya-production/emotion_references")

EMOTION_TEXTS = {
    "calm_supportive": "Take a deep breath. Everything will be okay.",
    "empathetic_sad": "I'm so sorry you're going through this.",
    "joyful_excited": "That's amazing! I'm so happy for you!",
    "playful": "Hey there! This is going to be fun!",
    "confident": "You've got this. I believe in you.",
    "concerned_anxious": "Are you okay? I'm here if you need me.",
    "angry_firm": "That's not acceptable. This needs to stop.",
    "neutral": "Hello. How can I help you today?"
}

@app.route('/health', methods=['GET'])
def health():
    return jsonify({
        'status': 'healthy',
        'model_loaded': True,
        'device': device,
        'gpu_memory_gb': torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0,
        'emotion_references_available': EMOTION_REF_DIR.exists(),
        'emotion_count': len(list(EMOTION_REF_DIR.glob("*.wav"))) if EMOTION_REF_DIR.exists() else 0
    })

@app.route('/generate', methods=['POST'])
def generate():
    try:
        data = request.json
        text = data.get('text', '')
        speaker = data.get('speaker', 0)
        context = data.get('context', [])
        max_audio_length_ms = data.get('max_audio_length_ms', 10000)
        reference_emotion = data.get('reference_emotion', None)

        print(f"ğŸ¤ Generating: {text[:50]}...")
        if reference_emotion:
            print(f"   ğŸ­ With emotion reference: {reference_emotion}")

        # Placeholder for actual CSM generation
        # Replace this with your actual CSM generation code
        print("   âš ï¸ CSM generation placeholder - implement your CSM logic here")
        
        # Generate dummy audio for testing
        sample_rate = 24000
        duration = len(text) * 0.1  # Rough estimate
        num_samples = int(duration * sample_rate)
        audio = torch.randn(num_samples) * 0.1
        
        buffer = io.BytesIO()
        torchaudio.save(buffer, audio.unsqueeze(0).cpu(), sample_rate, format='wav')
        buffer.seek(0)
        audio_bytes = buffer.read()
        audio_base64 = base64.b64encode(audio_bytes).decode('utf-8')

        duration_actual = audio.shape[-1] / sample_rate
        print(f"   âœ… Generated: {duration_actual:.2f}s")

        return jsonify({
            'audio_base64': audio_base64,
            'text': text,
            'speaker': speaker,
            'duration': duration_actual,
            'sample_rate': sample_rate,
            'reference_emotion': reference_emotion,
            'status': 'success'
        })

    except Exception as e:
        print(f"âŒ Generation failed: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    print('ğŸš€ Starting CSM server on port 19517...')
    print(f'ğŸ“ Emotion references directory: {EMOTION_REF_DIR}')
    app.run(host='0.0.0.0', port=19517)
EOF

# Create WhisperX test script
echo "ğŸ¤ Creating WhisperX test script..."
cat > /workspace/oviya-production/test_whisperx.py << 'EOF'
#!/usr/bin/env python3
"""
Test WhisperX functionality on RTX 5880 Ada
"""

import whisperx
import torch
import numpy as np
import time

def test_whisperx():
    print("ğŸ§ª Testing WhisperX on RTX 5880 Ada")
    print("=" * 50)
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Device: {device}")
    
    if torch.cuda.is_available():
        gpu_props = torch.cuda.get_device_properties(0)
        print(f"GPU: {gpu_props.name}")
        print(f"VRAM: {gpu_props.total_memory / 1e9:.1f}GB")
        print(f"CUDA: {torch.version.cuda}")
    
    # Load models
    print("\nğŸ“¥ Loading WhisperX models...")
    start_time = time.time()
    
    model = whisperx.load_model("large-v2", device, compute_type="float16")
    align_model, metadata = whisperx.load_align_model("en", device)
    
    load_time = time.time() - start_time
    print(f"âœ… Models loaded in {load_time:.2f}s")
    
    # Test with dummy audio
    print("\nğŸ¤ Testing transcription...")
    test_audio = np.random.randn(16000 * 3).astype(np.float32)  # 3 seconds
    
    start_time = time.time()
    result = model.transcribe(test_audio, batch_size=8)  # Large batch for RTX 5880
    transcribe_time = time.time() - start_time
    
    print(f"âœ… Transcription completed in {transcribe_time:.2f}s")
    print(f"   Batch size: 8")
    print(f"   Audio length: 3.0s")
    
    # Test alignment
    print("\nğŸ¯ Testing word-level alignment...")
    start_time = time.time()
    
    aligned_result = whisperx.align(
        result["segments"],
        align_model,
        metadata,
        test_audio,
        device,
        return_char_alignments=False
    )
    
    align_time = time.time() - start_time
    print(f"âœ… Alignment completed in {align_time:.2f}s")
    
    # Extract word timestamps
    word_count = 0
    for segment in aligned_result.get("segments", []):
        word_count += len(segment.get("words", []))
    
    print(f"   Word timestamps: {word_count}")
    
    # Memory usage
    if torch.cuda.is_available():
        memory_used = torch.cuda.memory_allocated() / 1e9
        memory_max = torch.cuda.max_memory_allocated() / 1e9
        print(f"\nğŸ’¾ Memory Usage:")
        print(f"   Current: {memory_used:.1f}GB")
        print(f"   Peak: {memory_max:.1f}GB")
    
    print("\nâœ… WhisperX test completed successfully!")
    print("ğŸ‰ RTX 5880 Ada is ready for production!")

if __name__ == "__main__":
    test_whisperx()
EOF

# Create Cloudflare tunnel setup script
echo "ğŸŒ Creating Cloudflare tunnel setup script..."
cat > /workspace/setup_tunnels.sh << 'EOF'
#!/bin/bash
echo "ğŸŒ Setting up Cloudflare tunnels for RTX 5880 Ada..."

# Install cloudflared
if ! command -v cloudflared &> /dev/null; then
    echo "ğŸ“¥ Installing cloudflared..."
    wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb
    dpkg -i cloudflared-linux-amd64.deb
    echo "âœ… cloudflared installed"
else
    echo "âœ… cloudflared already installed"
fi

# Create Ollama tunnel
echo "ğŸ”— Creating Ollama tunnel (port 11434)..."
cloudflared tunnel --url http://localhost:11434 &
OLLAMA_TUNNEL_PID=$!

# Wait a moment for tunnel to establish
sleep 5

# Create CSM tunnel
echo "ğŸ”— Creating CSM tunnel (port 19517)..."
cloudflared tunnel --url http://localhost:19517 &
CSM_TUNNEL_PID=$!

echo "âœ… Tunnels created!"
echo ""
echo "ğŸ“‹ Save these tunnel URLs when they appear:"
echo "   - Ollama: https://xxx.trycloudflare.com"
echo "   - CSM: https://yyy.trycloudflare.com"
echo ""
echo "ğŸ”§ To stop tunnels: kill $OLLAMA_TUNNEL_PID $CSM_TUNNEL_PID"
EOF

chmod +x /workspace/setup_tunnels.sh

# Create comprehensive test script
echo "ğŸ§ª Creating comprehensive test script..."
cat > /workspace/test_oviya_system.sh << 'EOF'
#!/bin/bash
echo "ğŸ§ª Testing Oviya System on RTX 5880 Ada"
echo "======================================"

cd /workspace/oviya-production

# Test GPU
echo "ğŸ” GPU Status:"
nvidia-smi --query-gpu=name,memory.total,memory.used --format=csv

# Test Ollama
echo ""
echo "ğŸ§  Testing Ollama..."
if curl -s http://localhost:11434/api/tags > /dev/null; then
    echo "âœ… Ollama is running"
    echo "ğŸ“‹ Available models:"
    curl -s http://localhost:11434/api/tags | python3 -m json.tool
else
    echo "âŒ Ollama is not responding"
fi

# Test CSM
echo ""
echo "ğŸ¤ Testing CSM..."
if curl -s http://localhost:19517/health > /dev/null; then
    echo "âœ… CSM server is running"
    echo "ğŸ“‹ CSM status:"
    curl -s http://localhost:19517/health | python3 -m json.tool
else
    echo "âŒ CSM server is not responding"
fi

# Test WhisperX
echo ""
echo "ğŸ¤ Testing WhisperX..."
python3 test_whisperx.py

echo ""
echo "ğŸ‰ System test completed!"
EOF

chmod +x /workspace/test_oviya_system.sh

# Test services
echo "ğŸ§ª Testing services..."

# Test Ollama
if curl -s http://localhost:11434/api/tags > /dev/null; then
    echo "âœ… Ollama is running"
else
    echo "âŒ Ollama failed to start"
fi

# Test WhisperX
echo "ğŸ¤ Testing WhisperX..."
python3 test_whisperx.py

echo ""
echo "ğŸ‰ Oviya EI + WhisperX setup complete on RTX 5880 Ada!"
echo ""
echo "ğŸ“‹ Services running:"
echo "   - Ollama LLM: http://localhost:11434"
echo "   - CSM Voice: http://localhost:19517 (placeholder)"
echo "   - Jupyter: http://localhost:8080"
echo ""
echo "ğŸ”§ Next steps:"
echo "   1. Run: /workspace/setup_tunnels.sh (to create Cloudflare tunnels)"
echo "   2. Run: /workspace/test_oviya_system.sh (to test everything)"
echo "   3. Implement your actual CSM model loading in csm_server_rtx5880.py"
echo ""
echo "ğŸ“ Project location: /workspace/oviya-production"
echo "ğŸ¯ RTX 5880 Ada (48GB VRAM) is ready for production!"
