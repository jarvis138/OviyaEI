# Oviya - Emotional Intelligence AI Companion

> *"Hey, I'm Ovia, your emotional intelligence companion. That means I'm here to help you navigate those wild and wonderful emotions we all feel."*

Oviya is a real-time voice AI companion that provides empathetic, emotionally-aware conversations. Built with a 4-layer architecture combining advanced AI models for natural human-like interaction.

## ğŸ—ï¸ Project Structure

```
oviya/
â”œâ”€â”€ core/                          # Shared core components
â”‚   â”œâ”€â”€ brain/                    # LLM brain components (personality, safety, etc.)
â”‚   â”œâ”€â”€ voice/                    # Voice processing (TTS, prosody, etc.)
â”‚   â”œâ”€â”€ data/                     # Data processing & bias filtering
â”‚   â”œâ”€â”€ monitoring/               # Analytics & metrics
â”‚   â”œâ”€â”€ validation/               # Emotion validation
â”‚   â”œâ”€â”€ serving/                  # API endpoints
â”‚   â””â”€â”€ rlhf/                     # Reinforcement learning
â”‚
â”œâ”€â”€ production/                   # Main production system (monolithic)
â”‚   â”œâ”€â”€ brain/                    # Production brain (uses core + extensions)
â”‚   â”œâ”€â”€ voice/                    # Production voice (uses core + extensions)
â”‚   â”œâ”€â”€ emotion_detector/         # Emotion detection
â”‚   â”œâ”€â”€ emotion_controller/       # Emotion mapping & control
â”‚   â”œâ”€â”€ websocket_server.py      # WebSocket API server
â”‚   â”œâ”€â”€ realtime_conversation.py  # Real-time pipeline
â”‚   â””â”€â”€ docker-compose.yml        # Production deployment
â”‚
â”œâ”€â”€ services/                     # Alternative microservices architecture
â”‚   â”œâ”€â”€ asr/                      # Speech recognition service
â”‚   â”œâ”€â”€ vad/                      # Voice activity detection
â”‚   â”œâ”€â”€ csm/                      # Voice synthesis service
â”‚   â”œâ”€â”€ orchestrator/             # Request orchestration
â”‚   â””â”€â”€ docker-compose.yml        # Services deployment
â”‚
â”œâ”€â”€ clients/                      # Client applications
â”‚   â”œâ”€â”€ mobile/                   # React Native mobile app
â”‚   â”œâ”€â”€ web/                      # Web client
â”‚   â”œâ”€â”€ website/                  # Public website
â”‚   â””â”€â”€ admin/                    # Admin interface
â”‚
â”œâ”€â”€ corpus/                       # Data processing pipeline
â”‚   â”œâ”€â”€ raw/                      # Raw emotional datasets
â”‚   â”œâ”€â”€ processed/                # Processed training data
â”‚   â””â”€â”€ scripts/                  # Processing scripts
â”‚
â”œâ”€â”€ mcp-ecosystem/               # Model Context Protocol ecosystem
â”‚   â”œâ”€â”€ monitoring/               # System monitoring & analytics
â”‚   â”œâ”€â”€ servers/                  # MCP server implementations
â”‚   â”‚   â”œâ”€â”€ tier1/               # Core thinking & reasoning
â”‚   â”‚   â”œâ”€â”€ tier2/               # Data & persistence services
â”‚   â”‚   â””â”€â”€ tier3/               # External integrations
â”‚   â””â”€â”€ config/                   # MCP configuration files
â”‚
â”œâ”€â”€ shared/                       # Infrastructure & deployment
â”‚   â”œâ”€â”€ docker/                   # Docker configurations
â”‚   â”œâ”€â”€ k8s/                      # Kubernetes manifests
â”‚   â””â”€â”€ ci/                       # CI/CD pipelines
â”‚
â”œâ”€â”€ tests/                        # Integration tests
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ docker-compose.yml            # Development orchestration
â”œâ”€â”€ .coderabbit.yml              # CodeRabbit AI review configuration
â””â”€â”€ README.md                     # This file
```

## ğŸš€ Quick Start

### Option 1: Production System (Recommended)

```bash
# 1. Install Ollama locally
brew install ollama
ollama serve &
ollama pull qwen2.5:7b

# 2. Start production system
cd production
docker-compose up -d

# 3. Access at http://localhost:8000
```

### Option 2: Development Setup

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Set environment variables
cp production/.env production/.env.local
# Edit .env.local with your service URLs

# 3. Run locally
cd production
python websocket_server.py
```

### Option 3: Microservices Architecture

```bash
cd services
docker-compose up -d
```

## ğŸ¯ Architecture Overview

### Production System (Monolithic)
- **WebSocket Server**: Real-time bidirectional communication
- **4-Layer Pipeline**:
  1. **Emotion Detector**: Analyzes user emotional state
  2. **Brain (LLM)**: Generates empathetic responses
  3. **Emotion Controller**: Maps emotions to acoustic parameters
  4. **Voice Engine**: Synthesizes emotional speech

### Microservices Alternative
- **ASR Service**: Speech-to-text processing
- **VAD Service**: Voice activity detection
- **CSM Service**: Voice synthesis
- **Orchestrator**: Request coordination

## ğŸ› ï¸ Key Components

### Core Shared Libraries
- **Bias Filtering**: Cultural sensitivity and safety
- **Personality System**: Long-term relationship memory
- **Prosody Engine**: Natural speech patterns
- **Monitoring**: Performance analytics

### Voice & Audio
- **CSM-1B**: Conversational speech synthesis
- **OpenVoice V2**: Voice cloning and fine control
- **Real-time Processing**: <500ms transcription latency

### Brain & Intelligence
- **Qwen2.5:7B**: Primary LLM for responses
- **49 Emotion Taxonomy**: Comprehensive emotional range
- **Context Awareness**: Conversation history and memory

## ğŸ“Š Performance

- **Transcription**: <500ms latency
- **Brain Processing**: <1.5s response time
- **Voice Synthesis**: <2s total
- **End-to-end**: <4s conversational turn

## ğŸ”§ Configuration

Environment variables control service endpoints:

```bash
# LLM Service
OLLAMA_URL=http://localhost:11434/api/generate

# Voice Services
CSM_URL=http://localhost:19517/generate
CSM_STREAM_URL=http://localhost:19517/generate/stream

# Speech Recognition
WHISPERX_URL=http://localhost:1111
```

## ğŸ¤– CodeRabbit AI Code Reviews

Oviya uses CodeRabbit for AI-powered code reviews that validate implementation against our therapeutic AI vision. Every pull request is automatically reviewed for:

### Vision Verification Rules
- **Unconditional Positive Regard**: Ensures non-judgmental, accepting language patterns
- **Secure Base Principles**: Validates attachment theory-based emotional responses
- **Vulnerability Reciprocity**: Checks appropriate reciprocal self-disclosure
- **Bid for Connection**: Verifies micro-affirmation responses to emotional bids
- **Global Wisdom Integration**: Ensures balance between Western psychology and global traditions
- **Safety & Ethics**: Validates locked fallback responses and audit trails

### Technical Excellence
- **Real-time Performance**: <500ms transcription, <4s end-to-end latency
- **GPU Memory Management**: Proper CUDA resource handling
- **Voice Data Privacy**: Encrypted processing with consent validation
- **Emotional Continuity**: Context preservation across conversation turns

### Setup
```bash
# Add to GitHub repository secrets
OPENAI_API_KEY=your_openai_api_key_here
```

**Documentation**: [CodeRabbit Setup Guide](.github/CODE_RABBIT_SETUP.md)

## ğŸ”„ MCP Ecosystem

Oviya includes a comprehensive Model Context Protocol (MCP) ecosystem for enhanced AI capabilities and system integration.

### Architecture Tiers

#### **Tier 1: Core Thinking & Reasoning**
- **Thinking Server**: Advanced cognitive empathy modes
- **Personality Engine**: Dynamic personality adaptation
- **Emotional Intelligence**: Deep emotional pattern recognition

#### **Tier 2: Data & Persistence**
- **PostgreSQL Server**: Structured data and conversation history
- **Redis Server**: High-performance caching and session state
- **Vector Storage**: Semantic search and memory retrieval

#### **Tier 3: External Integrations**
- **WhatsApp Server**: Social media integration
- **Stripe Server**: Payment processing and monetization
- **Analytics Server**: Usage metrics and performance monitoring

### Key Features
- **Modular Architecture**: Independent services with clear interfaces
- **Real-time Communication**: WebSocket-based inter-service messaging
- **Monitoring & Observability**: Comprehensive logging and metrics collection
- **Scalable Deployment**: Docker Compose and Kubernetes support

### Quick Start MCP
```bash
# Start the complete MCP ecosystem
cd mcp-ecosystem
docker-compose up -d

# Or start individual services
cd mcp-ecosystem/servers/tier2/postgres
docker-compose up -d
```

**Documentation**: [MCP Ecosystem Guide](mcp-ecosystem/README.md)

## ğŸ§ª Testing

```bash
# Run integration tests
python -m pytest tests/

# Test production system
cd production
python production_sanity_tests.py

# Test specific scenarios
python tests/test_5_scenarios.py
```

## ğŸ“¦ Deployment Options

### Docker (Recommended)
```bash
# Production deployment
cd production
docker-compose up -d

# Development with local Ollama
docker-compose --profile with-ollama up -d
```

### Kubernetes
```yaml
# Use manifests in shared/k8s/
kubectl apply -f shared/k8s/
```

## ğŸ¤ Contributing

1. **Core Components**: Changes to `core/` affect all systems
2. **Production**: Main development in `production/`
3. **Services**: Microservices development in `services/`
4. **Clients**: UI/UX development in `clients/`

## ğŸ“„ License

See individual component licenses.

## ğŸ”— Links

- [Architecture Documentation](production/README.md)
- [API Reference](production/websocket_server.py)
- [Client Integration](clients/README.md)
- [CodeRabbit Setup Guide](.github/CODE_RABBIT_SETUP.md)
- [MCP Ecosystem Guide](mcp-ecosystem/README.md)
- [MCP Configuration](mcp-ecosystem/config/oviya-mcp-config.json)




